*Experimentation*

To start with experimentation on web performance test for urls in virtual-labs and vlab using yslow.Our first need is collect the urls in *deploy.virtuals-labs.ac.in* and *www.vlab.co.in*.
* URL Collection
  For url collection,we needed to have some crawler to extract all the urls inside it.Therefore ,we started with *nutch*, but we could not install it properly on our system, so we left using *nutch* and   
queried on internet for sitemap generator.We got one sitemap generator which give 5000 urls under the seed url.Link for that website is http://www.internetmarketingninjas.com/seo-tools/google-sitemap-generator/.
Only thing we have to do is to give the seed url in it and it will give list of 5000 urls in excel format or Xml format.We downloaded report in XML format and wrote a script to store it in text file containing urls in each line.
Here in our case we have given seed url http://vlab.co.in. and http://deploy.virtual-labs.ac.in

Script to generate url file 
This script will take two command line arguments ,$1 will be given XML file and $2 will be output text file. 

#+begin_src :tangle a.py
grep -o '<loc>.*</loc>' $1 | sed 's/\(<loc>\|<\/loc>\)//g' > $2
#extracts all the urls which is present between <loc> and </loc>from given XML file i.e $1  and will stote it into the destination file i.e $2
#+end_src

* report generation using Yslow

For report generation  using yslow,firstly,we have to install *phantomjs* on machine. Phantomjs is the headless webkit with javascript APIs. *Yslow.js* runs on phantomjs. It can be install simply by following command.
#+begin_src :tangle a.py
sudo apt-get install phantomjs
#+end_src 

Then we download the yslow.js file from http://yslow.org/phantomjs/ and we save this file in the folder where url file is present.
To generate full report for each url ,we wrote a automated script which will read url line by line and will generate report for each url.
This script will take only one command line argument ,$1 i.e. file containing url

#+begin_src : tangle a.py

#function to run phantomjs for each url
Fulltest() 
{
while read url; do                    #loop reads a line from urls file i.e $1 
echo "Running tests for $url"         #echo on outputstream to indicate flow
dir=$(echo $url | sed 's/[:/.-]/_/g') #extracting filename 'dir' from 'url'	
phantomjs yslow.js --info grade --format tap --threshold '{"overall": "B", "ycdn": 65}' $url > $dir #running phantomjs command with url and redirecting output to a file named $dir
done < $1	#passing 'urls' file as parameter
}

Fulltest $1   #invoking Fulltest() function with command line arguments $1

#+end_src

Above script will generate report for each url present in the url file.These report will have overall score  out of 100 for each url and score out of 100 for each rule and suggestions on how to improve this scores.The sample report has been added to my bit-bucket account.
** Problem 
 Sometimes phantomjs crashes for some url and hence it doesn't generate report for that url.
 Also, as the number of urls increases ,these script takes larger time.
 We thought this was problem for a url i.e url is bad but we went to community and saw this is problem of phantomjs.
  
 
* Generating CSV file 
  For graph generation ,we need to have a CSV file which will contain in each line  name of the url,Overallscore,scores and scores for each rule separated with a *comma*.
  We wrote a automated script to generate CSV file extracting scores from each report.
  This script should be kept in the same folder where all the reports are present and the url file.

#+begin_src : tangle a.py

#FUNCTION TO EXTRACT REQUIRED FIELDS REPORTS PREVIOUSLY GENERATED
EXTRACTCSV() {
RES="URL,Overall_Score,Make fewer HTTP requests,Use a Content Delivery Network (CDN),Avoid empty src or href,Add Expires headers,Compress components with gzip,Put CSS at top,Put JavaScript at bottom,Avoid CSS expressions,Reduce DNS lookups,Minify JavaScript and CSS,Avoid URL redirects,Remove duplicate JavaScript and CSS,Configure entity tags (ETags),Make AJAX cacheable,Use GET for AJAX requests,Reduce the number of DOM elements,Avoid HTTP 404 (Not Found) error,Reduce cookie size,Use cookie-free domains,Avoid AlphaImageLoader filter,Do not scale images in HTML,Make favicon small and cacheable"
WHILE READ URL; DO #LOOP TO READ URL FROM 'URL' FILE
  RES="$RES"$'\N'
  RES="$RES$URL"
  DIR=$(ECHO $URL | SED 'S/[:/.-]/_/G') #EXTRACTING FILENAME 'DIR' FROM 'URL'
  RES="$RES,$(CAT $DIR | GREP -O '[A-Z] ([[:DIGIT:]]*)' | AWK -VORS=, '{ PRINT $1 $2 }' | SED 'S/,$/\N/')"
DONE < $1
ECHO "$RES" > STATS.CSV #REDIRECTING CONTENT IN 'RES' TO A CSV FILE NAMED STATS.CSV
ECHO "REPORTS ARE GENERATED SUCCESSFULLY IN CSV FORMAT"  #ECHOING SCRIPT COMPLETION       
}

#INVOKING EXTRACCSV() FUNTION
EXTRACTCSV $1

#+END_SRC

THIS SCRIPT WILL TAKE  ONE COMMAND LINE ARGUMENT,$1 I.E. URL FILE AND WILL STORE OUTPUT IN STATS.CSV FILE.

* GRAPH GENERATION
 
FOR GRAPH GENERATION,SOUMYA HAS WRITTEN  A AUTOMATED SCRIPT TO GENERATE ALL THE 24 GRAPHS.

* Statistics for urls under http:deploy.virtual-labs.ac.in
-  We collected around 21000 urls from the deploy server.
-  Then we ran the above script for fulltest to generate yslow report for each url.For some url its crashes on its own and it  took time around one day ,but it could generate report only 2700 urls.
-  So we stopped the script and will generate statistics for only these 2700 urls
-  Then we ran script to generate csv file.
-  After generating csv file for all the url,I removed those url from csv file for which there was no statistics because matplotlib will not plot graph for empty fields.
- After getting a fully correct csv, we plotted graphs for each rules and overall score.
- In these bar graphs, there is plot for no. of urls falling in each grade.
- These graphs gave us idea how good are web pages.and each graph also tells about how many urls are following best practices as we ave generated graph for each rule.

* Bar graph for landing pages of different educational websites.

we collected url of landing page  of *coursera.org*, *edx.org*, *nptel.ac.in* , *vlab.co.in*, *virtual-labs.ac.in* .
and ran the script for report generation for yslow.
After report generation, we made csv file for it.
But this time ,our aim was to show the difference between the overall scores of the landing page of educational web sites.
So we used only first field of csv file and generated graph for it.

* Statistics for 42 urls under cse14(VLSI lab) and bio16(Biomedical and Signal Processing Lab)

** No pagespeed

we collected 42 urls for the above mentioned two labs.
Then we ran our reportgeneration script to generate reports for yslow.
Then we ran our csv file generation script .
Now, for this csv file we plotted graph with respect to overall score.
from this graph , we observed that  out of 42,only 11 were in grade A,24 in grade B ,7 in grade C and none in other grades.

** When pagespeed on with default filters
This time ,we enabled pagespeed on our system with only default filters .
Then again we ran the same scripts to generate csv file.
Again,we plotted graph .
From this we observed that out of 42,27 url were in grade A,12 in grade B and 3 in grade C.
That is no.of url grade A increases.This means performance of web pages was improved by pagespeed.

** When pagespeed on with more filters
 This time , we enabled some more filters other than default filters.
 To enable we went to pagespeed.conf file which is located in etc/apaache2/mods-enabled folder and mods-avaliable folder. and enabled some filters like combine_css,collapse_whitespaces,rewrite_images,css_sprites.
 Again we ran our scripts to generate the CSV file.
 Again , we plotted graph for it. 
 From this ,we observed that out of 42 urls,26 url were in grade A,15 in grade B and only 1 in grade C.
That again pagespeed with its filters improves the performance of the web page.
