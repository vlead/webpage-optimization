			 *Testing Framework*

* *Introduction*
* *Background Knowledge*
** Linux Distributions    
*** Ubuntu
Debian based Linux operating system. It is most popular Linux distribution. Familiar with
commands of ubuntu. ex.sudo apt-get install python ex.sudo cp -r foldername destination and many more.
*** CentOS
Linux operating system. Comfortabl with yum commands.
** Apache server
Apache2 should be installed in the machine. Knowledge of apache configuration file is required. It is in /etc/apache2/apache2.conf .
** Shell Script
A shell script is a computer program designed to be run by the Unix shell, a command line interpreter. Basic understanding of shell scripting and bash commands.
** Matplotlib
Matplotlib is a python 2D plotting library which produces publication quality figures in a variety 
of hardcopy formats and interactive environments across platforms.
You can generate plots, histograms, power spectra, bar charts, errorcharts, scatterplots, etc, with just a few lines of code.
* *Investigation*
** Best practices
1.Make fewer HTTP requests.

2.Use a CDNs.

3.Avoid empty src or href

4.Add an Expires header

5.Compress components

6.Put CSS at top

7.Put Javascript at the bottom

8.Avoid CSS expression

9.Make JS and CSS external

10.Reduce DNS lookups

11.Minify JS and CSS

12.Avoid redirects

13.Remove duplicate JS and CSS

14.Configure ETags

15.Make Ajax cacheable

16.Use GET for AJAX requests

17.Reduce the Number of DOM elements

18.No 404s

19.Reduce Cookie Size

20.Use Cookie-free

21.Avoid filters

22.Don't Scale Images in HTML

23.Make favicon Small and Cacheable

For details: [[https://developer.yahoo.com/performance/rules.html][rules deta]]
** YSlow
*** YSlow.js
It runs on phantomjs.

Phantomjs is headless webkit with javascript api. Using best practices ,it has defined set of rules for which it generates scores and grade for each web practices and a overall score for a web page.
Each rule has certain weight for overall score.This report is in human readable format.
*** How YSlow.js works
1.It crawl the DOM.

2.It collects information about each component.

3.It generates grades and give suggestions for each rule which produces overall grade.

** Pagespeed
*** Module
This Google module not only analyses web pages performances but it automatically applies best practices to web pages
,so developer need not to change his contents.
*** Features
Free and open source.

Automatic web optimization. 

40+ filters to rewrite pages at run time.

Reduces response time.

Can be used for individual sites or CDNs.

*** Working
It has optimization filters in the pagespeed module written in C++.
Whenever a requests comes to server,this filters takes html as input and dynamically rewrites web pages to follow performance web practices and give optimized web pages.

For ex:See the working of *combine\_css* filters.
- Input
#+begin_src :tangle a.py
  <head>
    <link rel="stylesheet" type="text/css" href="styles/yellow.css">
    <link rel="stylesheet" type="text/css" href="styles/blue.css">
    <link rel="stylesheet" type="text/css" href="styles/big.css">
    <link rel="stylesheet" type="text/css" href="styles/bold.css">
  </head>
#+end_src

- Output

#+begin_src :tangle a.py
 <head>
    <link rel="stylesheet" type="text/css" href="styles/yellow.css+blue.css+big.css+bold.css.pagespeed.cc.xo4He3_gYf.css">
 </head>
#+end_src

Pagespeed can enabled and disabled acoording to our need.Also,Filters can be configured according to our use.
These filters  can be enabled and disabled whenver we want according to our requirements.
There is pagespeed.conf  file located in etc/apache2/mods-available and etc/apache2/mods-enabled folder.First make a link for both file so that change in one file effects change in other file.
Initially only some default core filters are enabled.Core filters are set of those which are safe for every websites.For list visit [[https://developers.google.com/speed/pagespeed/module/config_filters][Configuring filters]].
To enable pagespeed ,go to pagespeed.conf file and specify at the top

#+begin_src :tangle a.py
ModPagespeed on
#+end_src

To completely disable,go to pagespeed.conf file and specify at the top

#+begin_src :tangle a.py
ModPagespeed off
#+end_src


Filters can be enabled by specifying following command in the pagespeed.conf file.

For ex.we want to enable combine\_css filters,extend\_cache ,etc

#+begin_src :tangle a.py
ModPagespeedEnableFilters combine_css,extend_cache,rewrite_images
#+end_src  

For disabling  filter *a* and *b* specify

#+begin_src : tangle a.py
ModPagespeedDisableFilters filtera,filterb
#+end_src

After making any changes , you have to again compile mod\_pagespeed .Type the following command:

#+begin_src :tangle a.py

   cd ~/mod_pagespeed/src/install
   ./ubuntu.sh staging
   sudo ./ubuntu.sh install
   sudo ./ubuntu.sh stop start

#+end_src

For details of filters visit[[https://developers.google.com/speed/pagespeed/module/config_filters][ [[https://developers.google.com/speed/pagespeed/module/config_filters][Filters]]]]
*** Risks
There is some risk associated with every filters.It should be identified according to our use.Basically, it should be identified so that these filters should not change the semantics of page.

For ex. There is one filters defer\_javascript.It postpone the parallel execution of script tag.

        Calls to document.write fail in cases where they span multiple script tags.

        An example is:

#+begin_src :tangle a.py
<script>document.write('<div>')</script>
<span></span>
<script>document.write('</div>')</script> 
#+end_src 

* *Installation*
** Apache2
open terminal and type the following command.

#+BEGIN_SRC python :tangle a.py
#sudo apt-get update
#sudo apt-get install apache2
#sudo service apache2 start
#+END_SRC

Then open any browser and type 127.0.0.1

If it shows like this:
#+begin_src python :tangle a.py
It works
#+end_src 
Then your apache2 is installed.

** YSlow
Yslow runs on PhantomJS.

PhantomJS is headless webkit with javascript APIs.

1.To install PhantomJS ,go to terminal and type  :
#+begin_src: tangle a.py

$sudo apt-get install phantomjs
#+end_src

2.then go to http://yslow.org/phantomjs/ and download yslow for PhantomJS.

Extract it into folder.
** Pagespeed
Supported platforms.
 - CentOS/Fedora (32-bit and 64-bit)
 - Debian/Ubuntu (32-bit and 64-bit)
** Installation from packages
To install the packages, on Debian/Ubuntu, please run the following command:

#+begin_src: tangle a.py
$sudo dpkg -i mod-pagespeed-*.deb
$sudo apt-get -f install
#+end_src

For CentOS/Fedora, please execute:

#+begin_src :tangle a.py
$sudo yum install at  # if you do not already have 'at' installed
$sudo rpm -U mod-pagespeed-*.rpm
#+end_src

This will install latest updated version of modpagespeed.
*** Installation from packages
*** Installation form sources
**** Installing dependencies
To install these on Debian or Ubuntu run:

#+begin_src :tangle a.py
sudo apt-get install apache2 g++ python subversion gperf make devscripts fakeroot git
#+end_src
**** Installing the chromium depot tols.
We require the Chromium depot\_tools, which are used to build open-source projects with dependencies on other open-source projects. Download it with:

#+begin_src :tangle a.py
  mkdir -p ~/bin
  cd ~/bin
  svn co http://src.chromium.org/svn/trunk/tools/depot_tools
#+end_src
You will need to add the depot\_tools to your path. In bash you would run:
#+begin_src :tangle a.py 
export PATH=$PATH:~/bin/depot_tools
#+end_src

**** Check out mod_pagespeed and dependencies
You need to download the source code for mod\_pagespeed and all of its dependenceies. The gclient command (which is one of the depot\_tools) will do this for you:

#+begin_src :tangle a.py
  mkdir ~/mod_pagespeed    # Any directory is fine.
  cd ~/mod_pagespeed
#+end_src

Use it to get the latest stable version:
#+begin_src  :tangle a.py 
 gclient config http://modpagespeed.googlecode.com/svn/branches/latest-beta/src
 gclient sync --force --jobs=1
#+end_src

The current trunk uses https URLs to fetch some third party dependencies.

Depending on your system, you may need to verify and accept an updated SSL certificate for SourceForge. You can test by running:

#+begin_src :tangle a.py
svn ls https://svn.code.sf.net/p/jsoncpp/code/trunk/jsoncpp
#+end_src

If you see a message like the following, you'll need to accept the new certificate.
#+begin_src :tangle a.py
  Error validating server certificate for 'https://svn.code.sf.net:443':
   - The certificate is not issued by a trusted authority. Use the
     fingerprint to validate the certificate manually!
  Certificate information:
   - Hostname: *.code.sf.net
   - Valid: from Tue, 18 Mar 2014 00:00:00 GMT until Fri, 17 Apr 2015 23:59:59 GMT
   - Issuer: GeoTrust Inc., US
   - Fingerprint: 5e:d2:2a:09:0a:39:5e:f4:05:87:03:3a:13:2b:7d:52:3f:b8:1e:45
  (R)eject, accept (t)emporarily or accept (p)ermanently?
#+end_src


You should open https://svn.code.sf.net/p/jsoncpp/code/trunk/jsoncpp in a browser and verify that the fingerprint matches.

In Chrome, this can be done by clicking the lock icon next to the URL, clicking the "Certificate Information" link underneath the "Connection" tab, and verifying that the SHA-1 fingerprint matches.

**** Run tests
The commands below will first build mod\_pagespeed and then run the tests:

#+begin_src :tangle a.py
   cd ~/mod_pagespeed/src
   make AR.host=`pwd`/build/wrappers/ar.sh AR.target=`pwd`/build/wrappers/ar.sh \
      BUILDTYPE=Release mod_pagespeed_test pagespeed_automatic_test
   ./out/Release/mod_pagespeed_test
   ./out/Release/pagespeed_automatic_test 
#+end_src

**** Compile
To compile mod\_pagespeed, run:

#+begin_src :tangle a.py 
cd ~/mod_pagespeed/src
make AR.host=`pwd`/build/wrappers/ar.sh AR.target=`pwd`/build/wrappers/ar.sh BUILDTYPE=Release
#+end_src
**** Install
#+begin_src :tangle a.py
cd install
./install_apxs.sh

./ubuntu.sh staging
sudo ./ubuntu.sh install
sudo ./ubuntu.sh stop start
#+end_src
**** To check pagespeed is installed
Go to terminal and type command:
#+begin_src :tangle a.py
curl -D- http://localhost | less
#+end_src
You should get something like:
#+begin_src : tangle a.py
Date: Thu, 19 June 2014 19:16:32 GMT
Server: Apache/2.4.6 (Ubuntu)
...
X-Mod-Pagespeed: 1.8.31.3-4020
...
#+end_src
If you didn't get pagespeed in your http response header,then your web server isn't letting pagespeed to run or it is not installed properly or it is off from the pagespeed configuration file
 

*For more detailed information regarding installation ,you can visit:* [[https://developers.google.com/speed/pagespeed/module/build_mod_pagespeed_from_source#build-packages][pagespeed web-site]]

* *Experimentation*
To start with experimentation on web performance test for urls in virtual-labs and vlab using yslow.
Our first need is collect the urls in *deploy.virtuals-labs.ac.in* and *www.vlab.co.in*.
** URL collection
For url collection,we needed to have some crawler to extract all the urls inside it.Therefore ,we started with *nutch*, but we could not install it properly on our system, so we left using *nutch* and   
queried on internet for sitemap generator.We got one sitemap generator which give 5000 urls under the seed url.Link for that website is http://www.internetmarketingninjas.com/seo-tools/google-sitemap-generator/.
Only thing we have to do is to give the seed url in it and it will give list of 5000 urls in excel format or Xml format.We downloaded report in XML format and wrote a script to store it in text file containing urls in each line.
Here in our case we have given seed url http://vlab.co.in. and http://deploy.virtual-labs.ac.in

Script to generate url file 
This script will take two command line arguments ,$1 will be given XML file and $2 will be output text file. 

#+begin_src :tangle a.py
grep -o '<loc>.*</loc>' $1 | sed 's/\(<loc>\|<\/loc>\)//g' > $2
#extracts all the urls which is present between <loc> and </loc>from given XML file i.e $1  and will stote it into the destination file i.e $2
#+end_src
** Report generation using YSlow
For report generation  using yslow,firstly,we have to install *phantomjs* on machine. Phantomjs is the headless webkit with javascript APIs. *Yslow.js* runs on phantomjs. It can be install simply by following command.
#+begin_src :tangle a.py
sudo apt-get install phantomjs
#+end_src 

Then we download the yslow.js file from http://yslow.org/phantomjs/ and we save this file in the folder where url file is present.
To generate full report for each url ,we wrote a automated script which will read url line by line and will generate report for each url.
This script will take only one command line argument ,$1 i.e. file containing url

#+begin_src : tangle a.py

#function to run phantomjs for each url
Fulltest() 
{
while read url; do                    #loop reads a line from urls file i.e $1 
echo "Running tests for $url"         #echo on outputstream to indicate flow
dir=$(echo $url | sed 's/[:/.-]/_/g') #extracting filename 'dir' from 'url'	
phantomjs yslow.js --info grade --format tap --threshold '{"overall": "B", "ycdn": 65}' $url > $dir #running phantomjs command with url and redirecting output to a file named $dir
done < $1	#passing 'urls' file as parameter
}

Fulltest $1   #invoking Fulltest() function with command line arguments $1

#+end_src

Above script will generate report for each url present in the url file.These report will have overall score  out of 100 for each url and score out of 100 for each rule and
 suggestions on how to improve this scores
.The sample report has been added to my bit-bucket account.
*** Issues
 Sometimes phantomjs crashes for some url and hence it doesn't generate report for that url.
 Also, as the number of urls increases ,these script takes larger time.
 We thought this was problem for a url i.e url is bad but we went to community and saw this is problem of phantomjs.
** Generating CSV file
For graph generation ,we need to have a CSV file which will contain in each line  name of the url,Overallscore,scores and scores for each rule separated with a *comma*.
  We wrote a automated script to generate CSV file extracting scores from each report.
  This script should be kept in the same folder where all the reports are present and the url file.

#+begin_src : tangle a.py

#FUNCTION TO EXTRACT REQUIRED FIELDS REPORTS PREVIOUSLY GENERATED
EXTRACTCSV() {
RES="URL,OVERALL_SCORE,URL,Overall_Score,Make fewer HTTP requests,Use a Content Delivery Network (CDN),Avoid empty src or href,Add Expires headers,Compress components with gzip,Put CSS at top,Put JavaScript at bottom,Avoid CSS expressions,Reduce DNS lookups,Minify JavaScript and CSS,Avoid URL redirects,Remove duplicate JavaScript and CSS,Configure entity tags (ETags),Make AJAX cacheable,Use GET for AJAX requests,Reduce the number of DOM elements,Avoid HTTP 404 (Not Found) error,Reduce cookie size,Use cookie-free domains,Avoid AlphaImageLoader filter,Do not scale images in HTML,Make favicon small and cacheable"
WHILE READ URL; DO #LOOP TO READ URL FROM 'URL' FILE
  RES="$RES"$'\N'
  RES="$RES$URL"
  DIR=$(ECHO $URL | SED 'S/[:/.-]/_/G') #EXTRACTING FILENAME 'DIR' FROM 'URL'
  RES="$RES,$(CAT $DIR | GREP -O '[A-Z] ([[:DIGIT:]]*)' | AWK -VORS=, '{ PRINT $1 $2 }' | SED 'S/,$/\N/')"
DONE < $1
ECHO "$RES" > STATS.CSV #REDIRECTING CONTENT IN 'RES' TO A CSV FILE NAMED STATS.CSV
ECHO "REPORTS ARE GENERATED SUCCESSFULLY IN CSV FORMAT"  #ECHOING SCRIPT COMPLETION       
}

#INVOKING EXTRACCSV() FUNTION
EXTRACTCSV $1

#+END_SRC

THIS SCRIPT WILL TAKE  ONE COMMAND LINE ARGUMENT,$1 I.E. URL FILE AND WILL STORE OUTPUT IN STATS.CSV FILE.
** Graph Generation

** Statistics for URL under http://deploy.virtual-labs.ac.in
-  We collected around 21000 urls from the deploy server.
-  Then we ran the above script for fulltest to generate yslow report for each url.For some url its crashes on its own and it  took time around one day ,but it could generate report only 2700 urls.
-  So we stopped the script and will generate statistics for only these 2700 urls
-  Then we ran script to generate csv file.
-  After generating csv file for all the url,I removed those url from csv file for which there was no statistics because matplotlib will not plot graph for empty fields.
- After getting a fully correct csv, we plotted graphs for each rules and overall score.
- In these bar graphs, there is plot for no. of urls falling in each grade.
- These graphs gave us idea how good are web pages.and each graph also tells about how many urls are following best practices as we ave generated graph for each rule.

** Bar graph for landing page of different education websites
we collected url of landing page  of *coursera.org*, *edx.org*, *nptel.ac.in* , *vlab.co.in*, *virtual-labs.ac.in* .
and ran the script for report generation for yslow.
After report generation, we made csv file for it.
But this time ,our aim was to show the difference between the overall scores of the landing page of educational web sites.
So we used only first field of csv file and generated graph for it.

* *Analysis*
** Statistics for 42 URLs under cse14 (VLSI lab) and bio16 (Biomedical and signal processing lab)
*** Without Pagespeed 
we collected 42 urls for the above mentioned two labs.
Then we ran our reportgeneration script to generate reports for yslow.
Then we ran our csv file generation script .
Now, for this csv file we plotted graph with respect to overall score.
from this graph , we observed that  out of 42,only 11 were in grade A,24 in grade B ,7 in grade C and none in other grades.

*** With Pagespeed and with default filters
This time ,we enabled pagespeed on our system with only default filters .
Then again we ran the same scripts to generate csv file.
Again,we plotted graph .
From this we observed that out of 42,27 url were in grade A,12 in grade B and 3 in grade C.
That is no.of url grade A increases.This means performance of web pages was improved by pagespeed.

*** With Pagespeed and with more filters
This time , we enabled some more filters other than default filters.
 To enable we went to pagespeed.conf file which is located in etc/apaache2/mods-enabled folder and mods-avaliable folder. and enabled some filters like combine_css,collapse_whitespaces,rewrite_images,css_sprites.
 Again we ran our scripts to generate the CSV file.
 Again , we plotted graph for it. 
 From this ,we observed that out of 42 urls,26 url were in grade A,15 in grade B and only 1 in grade C.
That again pagespeed with its filters improves the performance of the web page.

** Technical Reviews 
** Further scope of project    
* *Conclusion*
* *References*
    
